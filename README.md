For efficiency, can install the flash-attn package
But requires building, need to make sure pytorch cuda version matches machine CUDA version

uv pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu124torch2.6-cp312-cp312-linux_x86_64.whl